{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a different approach to the cartpole question. I use Proximal Policy Optimation (PPO) to solve the cartpole problem. The actor maps the observation and the critic provides a value for the expected reward. The policy is updated with a stochastic gradient ascent optimizer. A stochastic gradient descent alogorithm fits the value function. Source: https://keras.io/examples/rl/ppo_cartpole/#visualizations   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: keras in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: gymnasium in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (3.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from keras) (0.14.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/rlenv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy tensorflow keras gymnasium scipy matplotlib  #only works with python11 or older"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent values future rewards but at a discounted rate.\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing agents experiences so we can use them later for training.\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        if self.pointer >= len(self.observation_buffer):  # Prevent overflow\n",
    "            print(f\"Warning: Buffer is full. Resetting pointer.\")\n",
    "            return  # Stop storing if buffer is full\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating feedforward nuerel network\n",
    "def mlp(x, sizes, activation=keras.activations.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = keras.ops.log_softmax(logits)\n",
    "    logprobability = keras.ops.sum(\n",
    "        keras.ops.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "seed_generator = keras.random.SeedGenerator(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes an observation and provides feedback from the policy.\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = keras.ops.squeeze(\n",
    "        keras.random.categorical(logits, 1, seed=seed_generator), axis=1\n",
    "    )\n",
    "    return logits, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actor: trains the policy -- ppo clips policy to prevent large updates that will destabilize learning.\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = keras.ops.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = keras.ops.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -keras.ops.mean(\n",
    "            keras.ops.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = keras.ops.mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = keras.ops.sum(kl)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#critic: minimizes difference between predicted and actual value -- updates weights to make future predictions more accurate\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = keras.ops.mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 4000\n",
    "epochs = 30\n",
    "gamma = 0.99\n",
    "clip_ratio = 0.3\n",
    "policy_learning_rate = 5e-4\n",
    "value_function_learning_rate = 1e-3\n",
    "train_policy_iterations = 150\n",
    "train_value_iterations = 80\n",
    "lam = 0.97\n",
    "target_kl = 0.02\n",
    "hidden_sizes = (64, 64)\n",
    "\n",
    "# True to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Actor (Policy Network): Maximizes expected rewards by improving actions.\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=\"float32\")\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions])\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "\n",
    "# Critic (Value Function): Helps stabilize learning by reducing variance.\n",
    "value = keras.ops.squeeze(mlp(observation_input, list(hidden_sizes) + [1]), axis=1)\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Optimizers: Adaptive Moment Estimation (Adam)\n",
    "policy_optimizer = keras.optimizers.Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = keras.optimizers.Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, _ = env.reset()\n",
    "episode_return, episode_length = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 1. Mean Return: 21.978021978021978. Mean Length: 21.978021978021978\n",
      " Epoch: 2. Mean Return: 37.735849056603776. Mean Length: 37.735849056603776\n",
      " Epoch: 3. Mean Return: 68.96551724137932. Mean Length: 68.96551724137932\n",
      " Epoch: 4. Mean Return: 117.6470588235294. Mean Length: 117.6470588235294\n",
      " Epoch: 5. Mean Return: 200.0. Mean Length: 200.0\n",
      " Epoch: 6. Mean Return: 142.85714285714286. Mean Length: 142.85714285714286\n",
      " Epoch: 7. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n",
      " Epoch: 8. Mean Return: 222.22222222222223. Mean Length: 222.22222222222223\n",
      " Epoch: 9. Mean Return: 1333.3333333333333. Mean Length: 1333.3333333333333\n",
      " Epoch: 10. Mean Return: 333.3333333333333. Mean Length: 333.3333333333333\n",
      " Epoch: 11. Mean Return: 166.66666666666666. Mean Length: 166.66666666666666\n",
      " Epoch: 12. Mean Return: 400.0. Mean Length: 400.0\n",
      " Epoch: 13. Mean Return: 181.8181818181818. Mean Length: 181.8181818181818\n",
      " Epoch: 14. Mean Return: 800.0. Mean Length: 800.0\n",
      " Epoch: 15. Mean Return: 2000.0. Mean Length: 2000.0\n",
      " Epoch: 16. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 17. Mean Return: 210.52631578947367. Mean Length: 210.52631578947367\n",
      " Epoch: 18. Mean Return: 266.6666666666667. Mean Length: 266.6666666666667\n",
      " Epoch: 19. Mean Return: 444.44444444444446. Mean Length: 444.44444444444446\n",
      " Epoch: 20. Mean Return: 1000.0. Mean Length: 1000.0\n",
      " Epoch: 21. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 22. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 23. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 24. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 25. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 26. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 27. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 28. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 29. Mean Return: 4000.0. Mean Length: 4000.0\n",
      " Epoch: 30. Mean Return: 4000.0. Mean Length: 4000.0\n",
      "Rewards saved at: /tmp/rewards_per_epoch.npy\n"
     ]
    }
   ],
   "source": [
    "# Initialize rewards_per_epoch before training\n",
    "rewards_per_epoch = []\n",
    "\n",
    "# Iterate over the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "    episode_return = 0 \n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            observation, _ = env.reset()\n",
    "            episode_return, episode_length = 0, 0\n",
    "\n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {sum_return / num_episodes}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )\n",
    "    rewards_per_epoch.append(sum_return / max(num_episodes, 1))  #Avoid ZeroDivisionError\n",
    "\n",
    "\n",
    "save_path = \"/tmp/rewards_per_epoch.npy\"  # Use a writable directory\n",
    "\n",
    "# check if directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save rewards only after all epochs are completed\n",
    "np.save(save_path, np.array(rewards_per_epoch))\n",
    "\n",
    "print(f\"Rewards saved at: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Plot saved at: /tmp/ppo_training.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Use a non-GUI backend for GitHub Actions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the correct file path (Modify based on your saving location)\n",
    "save_path = \"/tmp/rewards_per_epoch.npy\"  # üîπ Use this if stored in /tmp/\n",
    "# save_path = \"/github/workspace/rewards_per_epoch.npy\"  # üîπ Use this if running on GitHub Actions\n",
    "\n",
    "# Ensure the file exists before loading to prevent crashes\n",
    "if os.path.exists(save_path):\n",
    "    rewards_per_epoch = np.load(save_path)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"‚ö†Ô∏è File not found: {save_path}\")\n",
    "\n",
    "# Generate the training visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(rewards_per_epoch) + 1), rewards_per_epoch, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Return\")\n",
    "plt.title(\"PPO Training Performance on CartPole-v1\")\n",
    "plt.grid()\n",
    "plt.ylim(bottom=0, top=max(rewards_per_epoch) * 1.2 + 1)\n",
    "\n",
    "# Save the plot in a writable location\n",
    "output_path = \"/tmp/ppo_training.png\"  # üîπ Adjust based on the environment\n",
    "# output_path = \"/github/workspace/ppo_training.png\"  # üîπ Use this if in GitHub Actions\n",
    "\n",
    "plt.savefig(output_path, dpi=300)\n",
    "\n",
    "print(f\" Plot saved at: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
